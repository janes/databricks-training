{"cells":[{"cell_type":"markdown","source":["# Aggregations and JOINs\nApache Spark&trade; and Databricks&reg; allow you to create on-the-fly data lakes."],"metadata":{}},{"cell_type":"markdown","source":["### Getting Started\n\nRun the following cell to configure our \"classroom.\""],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["-sandbox\n## Basic Aggregations\n\nUsing <a \"https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions\" target=\"_blank\">built-in Spark functions</a>, you can aggregate data in various ways. \n\nRun the cell below to compute the average of all salaries in the people DataFrame.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> By default, you get a floating point value."],"metadata":{}},{"cell_type":"code","source":["peopleDF = spark.read.parquet(\"/mnt/training/dataframes/people-10m.parquet\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["from pyspark.sql.functions import avg\navgSalaryDF = peopleDF.select(avg(\"salary\").alias(\"averageSalary\"))\n\navgSalaryDF.show()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["Convert that value to an integer using the `round()` function. See\n<a href \"https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\" class=\"text-info\">the documentation for <tt>round()</tt></a>\nfor more details."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import round\nroundedAvgSalaryDF = avgSalaryDF.select(round(\"averageSalary\").alias(\"roundedAverageSalary\"))\n\nroundedAvgSalaryDF.show()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["In addition to the average salary, what are the maximum and minimum salaries?"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import min, max\nsalaryDF = peopleDF.select(max(\"salary\").alias(\"max\"), min(\"salary\").alias(\"min\"), round(avg(\"salary\")).alias(\"averageSalary\"))\n\nsalaryDF.show()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["## Joining Two Data Sets\n\nCorrelate the data in two data sets using a DataFrame join. \n\nThe `people` data set has 10 million names in it. \n\n> How many of the first names appear in Social Security data files? \n\nTo find out, use the Social Security data set with first name popularity data from the United States Social Security Administration. \n\nFor every year from 1880 to 2014, `dbfs:/mnt/training/ssn/names-1880-2016.parquet/` lists the first names of people born in that year, their gender, and the total number of people given that name. \n\nBy joining the `people` data set with `names-1880-2016`, weed out the names that aren't represented in the Social Security data.\n\n(In a real application, you might use a join like this to filter out bad data.)"],"metadata":{}},{"cell_type":"markdown","source":["Start by taking a look at what the social security data set looks like. Each year is its own directory."],"metadata":{}},{"cell_type":"code","source":["%fs ls dbfs:/mnt/training/ssn/names-1880-2016.parquet/"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["Let's load this file into a DataFrame and look at the data."],"metadata":{}},{"cell_type":"code","source":["ssaDF = spark.read.parquet(\"/mnt/training/ssn/names-1880-2016.parquet/\")\n\ndisplay(ssaDF)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["Next, with a quick count of distinct names, get an idea of how many distinct names there are in each of the tables.\n\nDataFrames have a `distinct` method just for this purpose."],"metadata":{}},{"cell_type":"code","source":["peopleDistinctNamesDF = peopleDF.select(\"firstName\").distinct()"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["peopleDistinctNamesDF.count()"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["-sandbox\nIn preparation for the join, let's rename the `firstName` column to `ssaFirstName` in the Social Security DataFrame.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Question to ponder: why would we want to do this?"],"metadata":{}},{"cell_type":"code","source":["ssaDistinctNamesDF = ssaDF.select(\"firstName\").withColumnRenamed(\"firstName\",'ssaFirstName').distinct()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["Count how many distinct names in the Social Security DataFrame."],"metadata":{}},{"cell_type":"code","source":["ssaDistinctNamesDF.count()"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["Now join the two DataFrames."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col\njoinedDF = peopleDistinctNamesDF.join(ssaDistinctNamesDF, col(\"firstName\") == col(\"ssaFirstName\"))"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["How many are there?"],"metadata":{}},{"cell_type":"code","source":["joinedDF.count()"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["-sandbox\n## Exercise 1\n\nIn the tables above, some of the salaries in the `peopleDF` DataFrame are negative. \n\nThese salaries represent bad data. \n\nYour job is to convert all the negative salaries to positive ones, and then sort the top 20 people by their salary.\n\n<img alt=\"Hint\" title=\"Hint\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-light-bulb.svg\"/>&nbsp;**Hint:** See the Apache Spark documentation, <a href=\"https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\" target=\"_blank\">built-in functions</a>."],"metadata":{}},{"cell_type":"markdown","source":["### Step 1\nCreate a DataFrame`PeopleWithFixedSalariesDF`, where all the negative salaries have been converted to positive numbers."],"metadata":{}},{"cell_type":"code","source":["# TODO\n\nfrom pyspark.sql.functions import abs\npeopleWithFixedSalariesDF = # FILL_IN"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\n\nbelowZero = peopleWithFixedSalariesDF.filter(peopleWithFixedSalariesDF[\"salary\"] < 0).count()\ndbTest(\"DF-L3-belowZero\", 0, belowZero)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["### Step 2\n\nStarting with the `peopleWithFixedSalariesDF` DataFrame, create another DataFrame called `PeopleWithFixedSalariesSortedDF` where:\n0. The data set has been reduced to the first 20 records.\n0. The records are sorted by the column `salary` in ascending order."],"metadata":{}},{"cell_type":"code","source":["# TODO\npeopleWithFixedSalariesSortedDF = # FILL_IN"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\n\nresultsDF = peopleWithFixedSalariesSortedDF.select(\"salary\")\ndbTest(\"DF-L3-count\", 20, resultsDF.count())\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\n\nfrom pyspark.sql import Row\n\nresults = resultsDF.collect()\n\ndbTest(\"DF-L3-fixedSalaries-0\", Row(salary=2), results[0])\ndbTest(\"DF-L3-fixedSalaries-1\", Row(salary=3), results[1])\ndbTest(\"DF-L3-fixedSalaries-2\", Row(salary=4), results[2])\n\ndbTest(\"DF-L3-fixedSalaries-10\", Row(salary=19), results[10])\ndbTest(\"DF-L3-fixedSalaries-11\", Row(salary=19), results[11])\ndbTest(\"DF-L3-fixedSalaries-12\", Row(salary=20), results[12])\n\ndbTest(\"DF-L3-fixedSalaries-17\", Row(salary=28), results[17])\ndbTest(\"DF-L3-fixedSalaries-18\", Row(salary=30), results[18]) \ndbTest(\"DF-L3-fixedSalaries-19\", Row(salary=31), results[19]) \n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["## Exercise 2\n\nAs a refinement, assume all salaries under $20,000 represent bad rows and filter them out.\n\nAdditionally, categorize each person's salary into $10K groups."],"metadata":{}},{"cell_type":"markdown","source":["### Step 1\n Starting with the `peopleWithFixedSalariesDF` DataFrame, create a DataFrame called `peopleWithFixedSalaries20KDF` where:\n0. The data set excludes all records where salaries are below $20K.\n0. The data set includes a new column called `salary10k`, that should be the salary in groups of 10,000. For example:\n  * A salary of 23,000 should report a value of \"2\".\n  * A salary of 57,400 should report a value of \"6\".\n  * A salary of 1,231,375 should report a value of \"123\"."],"metadata":{}},{"cell_type":"code","source":["# TODO\npeopleWithFixedSalaries20KDF = # FILL_IN"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\n\nbelow20K = peopleWithFixedSalaries20KDF.filter(\"salary < 20000\").count()\n \ndbTest(\"DF-L3-count-salaries\", 0, below20K)  \n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\n\nfrom pyspark.sql.functions import count\nresults = (peopleWithFixedSalaries20KDF \n  .select(\"salary10k\") \n  .groupBy(\"salary10k\") \n  .agg(count(\"*\").alias(\"total\")) \n  .orderBy(\"salary10k\") \n  .limit(5) \n  .collect()\n)\n\ndbTest(\"DF-L3-countSalaries-0\", Row(salary10k=2.0, total=43792), results[0])\ndbTest(\"DF-L3-countSalaries-1\", Row(salary10k=3.0, total=212630), results[1])\ndbTest(\"DF-L3-countSalaries-2\", Row(salary10k=4.0, total=536536), results[2])\ndbTest(\"DF-L3-countSalaries-3\", Row(salary10k=5.0, total=1055261), results[3])\ndbTest(\"DF-L3-countSalaries-4\", Row(salary10k=6.0, total=1623248), results[4])\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":["## Exercise 3\n\nUsing the `peopleDF` DataFrame, count the number of females named Caren who were born before March 1980."],"metadata":{}},{"cell_type":"markdown","source":["### Step 1\n\nStarting with `peopleDF`, create a DataFrame called `carensDF` where:\n0. The result set has a single record.\n0. The data set has a single column named `total`.\n0. The result counts only \n  * Females (`gender`)\n  * First Name is \"Caren\" (`firstName`)\n  * Born before March 1980 (`birthDate`)"],"metadata":{}},{"cell_type":"code","source":["# TODO\ncarensDF = peopleDF.filter(\"gender == 'F'\").filter(\"firstName == 'Caren'\").filter(\"birthDate > 1980\")\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o297.filter.\n: org.apache.spark.sql.AnalysisException: cannot resolve &#39;(`birthDate` &gt; 1980)&#39; due to data type mismatch: differing types in &#39;(`birthDate` &gt; 1980)&#39; (timestamp and int).; line 1 pos 0;\n&#39;Filter (birthDate#7996 &gt; 1980)\n+- Filter (firstName#7992 = Caren)\n   +- Filter (gender#7995 = F)\n      +- Relation[id#7991,firstName#7992,middleName#7993,lastName#7994,gender#7995,birthDate#7996,ssn#7997,salary#7998] parquet\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:94)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:94)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$3.apply(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$3.apply(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:117)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$4.apply(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:94)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:89)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:147)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:89)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:103)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:117)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:114)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:114)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:82)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:82)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:82)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.Dataset.&lt;init&gt;(Dataset.scala:187)\n\tat org.apache.spark.sql.Dataset.&lt;init&gt;(Dataset.scala:193)\n\tat org.apache.spark.sql.Dataset$.apply(Dataset.scala:66)\n\tat org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:3532)\n\tat org.apache.spark.sql.Dataset.filter(Dataset.scala:1511)\n\tat org.apache.spark.sql.Dataset.filter(Dataset.scala:1525)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nDuring handling of the above exception, another exception occurred:\n\n<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-2608287324245224&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-red-fg\"># TODO</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\"> </span>carensDF <span class=\"ansi-blue-fg\">=</span> peopleDF<span class=\"ansi-blue-fg\">.</span>filter<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;gender == &#39;F&#39;&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>filter<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;firstName == &#39;Caren&#39;&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>filter<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;birthDate &gt; 1980&#34;</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">filter</span><span class=\"ansi-blue-fg\">(self, condition)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1385</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">   1386</span>         <span class=\"ansi-green-fg\">if</span> isinstance<span class=\"ansi-blue-fg\">(</span>condition<span class=\"ansi-blue-fg\">,</span> basestring<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">-&gt; 1387</span><span class=\"ansi-red-fg\">             </span>jdf <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>filter<span class=\"ansi-blue-fg\">(</span>condition<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1388</span>         <span class=\"ansi-green-fg\">elif</span> isinstance<span class=\"ansi-blue-fg\">(</span>condition<span class=\"ansi-blue-fg\">,</span> Column<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1389</span>             jdf <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>filter<span class=\"ansi-blue-fg\">(</span>condition<span class=\"ansi-blue-fg\">.</span>_jc<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     67</span>                                              e.java_exception.getStackTrace()))\n<span class=\"ansi-green-intense-fg ansi-bold\">     68</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.AnalysisException: &#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 69</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     70</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.catalyst.analysis&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     71</span>                 <span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AnalysisException</span>: &#34;cannot resolve &#39;(`birthDate` &gt; 1980)&#39; due to data type mismatch: differing types in &#39;(`birthDate` &gt; 1980)&#39; (timestamp and int).; line 1 pos 0;\\n&#39;Filter (birthDate#7996 &gt; 1980)\\n+- Filter (firstName#7992 = Caren)\\n   +- Filter (gender#7995 = F)\\n      +- Relation[id#7991,firstName#7992,middleName#7993,lastName#7994,gender#7995,birthDate#7996,ssn#7997,salary#7998] parquet\\n&#34;</div>"]}}],"execution_count":42},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\n\nrows = carensDF.collect()\n\ndbTest(\"DF-L3-carens-len\", 1, len(rows))\ndbTest(\"DF-L3-carens-total\", Row(total=750), rows[0])\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":["## Review Questions\n**Q:** What is the DataFrame equivalent of the SQL statement `SELECT count(*) AS total`  \n**A:** ```.agg(count(\"*\").alias(\"total\"))```\n\n**Q:** What is the DataFrame equivalent of the SQL statement \n```SELECT firstName FROM PeopleDistinctNames INNER JOIN SSADistinctNames ON firstName = ssaFirstName```  \n**A:** \n`peopleDistinctNamesDF.join(ssaDistinctNamesDF, peopleDistinctNamesDF(col(\"firstName\")) == col(\"ssaFirstName\"))`"],"metadata":{}},{"cell_type":"markdown","source":["## Next Steps\n\n* Do the [Challenge Exercise]($./Optional/03-Joins-Aggregations).\n* Start the next lesson, [Accessing Data]($./04-Accessing-Data)."],"metadata":{}},{"cell_type":"markdown","source":["## Additional Topics & Resources\n\n* <a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html\" target=\"_blank\">Spark SQL, DataFrames and Datasets Guide</a>\n* <a href=\"https://databricks.com/blog/2017/08/31/cost-based-optimizer-in-apache-spark-2-2.html\" target=\"_blank\">Cost-based Optimizer in Apache Spark 2.2</a>"],"metadata":{}}],"metadata":{"name":"03-Joins-Aggregations","notebookId":2608287324245182},"nbformat":4,"nbformat_minor":0}
