{"cells":[{"cell_type":"markdown","source":["# Custom Transformations, Aggregating and Loading\n\nThe goal of this project is to populate aggregate tables using Twitter data.  In the process, you write custom User Defined Functions (UDFs), aggregate daily most trafficked domains, join new records to a lookup table, and load to a target database.\n\nIn this project you ETL JSON Twitter data to build aggregate tables that monitor trending websites and hashtags and filter malicious users using historical data.  Use these four exercises to achieve this goal:<br><br>\n\n1. **Parse tweeted URLs** using a custom UDF\n2. **Compute aggregate statistics** of most tweeted websites and hashtags by day\n3. **Join new data** to an existing dataset of malicious users\n4. **Load records** into a target database"],"metadata":{}},{"cell_type":"markdown","source":["Run the following cell."],"metadata":{}},{"cell_type":"code","source":["%run \"../Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["## Exercise 1: Parse Tweeted URLs\n\nSome tweets in the dataset contain links to other websites.  Import and explore the dataset using the provided schema.  Then, parse the domain name from these URLs using a custom UDF."],"metadata":{}},{"cell_type":"markdown","source":["### Step 1: Import and Explore\n\nThe following is the schema created as part of the capstone project for ETL Part 1.  \nRun the following cell and then use this schema to import one file of the Twitter data."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import StructField, StructType, ArrayType, StringType, IntegerType, LongType\nfrom pyspark.sql.functions import col\n\nfullTweetSchema = StructType([\n  StructField(\"id\", LongType(), True),\n  StructField(\"user\", StructType([\n    StructField(\"id\", LongType(), True),\n    StructField(\"screen_name\", StringType(), True),\n    StructField(\"location\", StringType(), True),\n    StructField(\"friends_count\", IntegerType(), True),\n    StructField(\"followers_count\", IntegerType(), True),\n    StructField(\"description\", StringType(), True)\n  ]), True),\n  StructField(\"entities\", StructType([\n    StructField(\"hashtags\", ArrayType(\n      StructType([\n        StructField(\"text\", StringType(), True)\n      ]),\n    ), True),\n    StructField(\"urls\", ArrayType(\n      StructType([\n        StructField(\"url\", StringType(), True),\n        StructField(\"expanded_url\", StringType(), True),\n        StructField(\"display_url\", StringType(), True)\n      ]),\n    ), True)\n  ]), True),\n  StructField(\"lang\", StringType(), True),\n  StructField(\"text\", StringType(), True),\n  StructField(\"created_at\", StringType(), True)\n])"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["Import one file of the data located at `/mnt/training/twitter/firehose/2018/01/08/18/twitterstream-1-2018-01-08-18-48-00-bcf3d615-9c04-44ec-aac9-25f966490aa4` using the schema.  Be sure to do the following:<br><br>\n\n* Save the result to `tweetDF`\n* Apply the schema `fullTweetSchema`\n* Filter out null values from the `id` column"],"metadata":{}},{"cell_type":"code","source":["# TODO\npath = # FILL_IN\ntweetDF = # FILL_IN"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\ndbTest(\"ET2-P-08-01-01\", 1491, tweetDF.count())\ndbTest(\"ET2-P-08-01-02\", True, \"text\" in tweetDF.columns and \"id\" in tweetDF.columns)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["-sandbox\n### Step 2: Write a UDF to Parse URLs\n\nThe Python regular expression library `re` allows you to define a set of rules of a string you want to match. In this case, parse just the domain name in the string for the URL of a link in a Tweet. Take a look at the following example:\n\n```\nimport re\n\nURL = \"https://www.databricks.com/\"\npattern = re.compile(r\"https?://(www\\.)?([^/#?]+).*$\")\nmatch = pattern.search(URL)\nprint(\"The string {} matched {}\".format(URL, match.group(2)))\n```\n\nThis code prints `The string https://www.databricks.com/ matched spark.apache.org`. **Wrap this code into a function named `getDomain` that takes a parameter `URL` and returns the matched string.**\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> <a href=\"https://docs.python.org/3/howto/regex.html\" target=\"_blank\">You can find more on the `re` library here.</a>"],"metadata":{}},{"cell_type":"code","source":["# TODO\ndef getDomain(URL):\n  # FILL_IN\n\nURL = \"https://www.databricks.com/\"\nprint(\"The string {} matched {}\".format(URL, getDomain(URL)))"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\ndbTest(\"ET2-P-08-02-01\", \"databricks.com\",  getDomain(\"https://www.databricks.com/\"))\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["### Step 3: Test and Register the UDF\n\nNow that the function works with a single URL, confirm that it works on different URL formats."],"metadata":{}},{"cell_type":"markdown","source":["Run the following cell to test your function further."],"metadata":{}},{"cell_type":"code","source":["urls = [\n  \"https://www.databricks.com/\",\n  \"https://databricks.com/\",\n  \"https://databricks.com/training-overview/training-self-paced\",\n  \"http://www.databricks.com/\",\n  \"http://databricks.com/\",\n  \"http://databricks.com/training-overview/training-self-paced\",\n  \"http://www.apache.org/\",\n  \"http://spark.apache.org/docs/latest/\"\n]\n\nfor url in urls:\n  print(getDomain(url))"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["Register the UDF as `getDomainUDF`."],"metadata":{}},{"cell_type":"code","source":["# TODO\ngetDomainUDF = # FILL_IN"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\ndbTest(\"ET2-P-08-03-01\", True, bool(getDomainUDF))\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["### Step 4: Apply the UDF\n\nCreate a dataframe called `urlDF` that has three columns:<br><br>\n\n1. `URL`: The URL's from `tweetDF` (located in `entities.urls.expanded_url`) \n2. `parsedURL`: The UDF applied to the column `URL`\n3. `created_at`\n\nThere can be zero, one, or many URLs in any tweet.  For this step, use the `explode` function, which takes an array like URLs and returns one row for each value in the array.  <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=explode#pyspark.sql.functions.explode\" target=\"_blank\">See the documents here for details.</a>"],"metadata":{}},{"cell_type":"code","source":["# TODO\nurlDF = # FILL_IN"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\ncols = urlDF.columns\nsample = urlDF.first()\n\ndbTest(\"ET2-P-08-04-01\", True, \"URL\" in cols and \"parsedURL\" in cols and \"created_at\" in cols)\ndbTest(\"ET2-P-08-04-02\", \"https://www.youtube.com/watch?v=b4iz9nZPzAA\", sample[\"URL\"])\ndbTest(\"ET2-P-08-04-03\", \"Mon Jan 08 18:47:59 +0000 2018\", sample[\"created_at\"])\ndbTest(\"ET2-P-08-04-04\", \"youtube.com\", sample[\"parsedURL\"])\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["## Exercise 2: Compute Aggregate Statistics\n\nCalculate top trending 10 URLs by hour."],"metadata":{}},{"cell_type":"markdown","source":["### Step 1: Parse the Timestamp\n\nCreate a DataFrame `urlWithTimestampDF` that includes the following columns:<br><br>\n\n* `URL`\n* `parsedURL`\n* `timestamp`\n* `hour`\n\nImport `unix_timestamp` and `hour` from the `functions` module and `TimestampType` from the types `module`. To parse the `create_at` field, use `unix_timestamp` with the format `EEE MMM dd HH:mm:ss ZZZZZ yyyy`."],"metadata":{}},{"cell_type":"code","source":["# TODO\ntimestampFormat = \"EEE MMM dd HH:mm:ss ZZZZZ yyyy\"\nurlWithTimestampDF = # FILL_IN"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\ncols = urlWithTimestampDF.columns\nsample = urlWithTimestampDF.first()\n\ndbTest(\"ET2-P-08-05-01\", True, \"URL\" in cols and \"parsedURL\" in cols and \"timestamp\" in cols and \"hour\" in cols)\ndbTest(\"ET2-P-08-05-02\", 18, sample[\"hour\"])\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["### Step 2: Calculate Trending URLs\n\nCreate a DataFrame `urlTrendsDF` that looks at the top 10 hourly counts of domain names and includes the following columns:<br><br>\n\n* `hour`\n* `parsedURL`\n* `count`\n\nThe result should sort `hour` in ascending order and `count` in descending order."],"metadata":{}},{"cell_type":"code","source":["# TODO\nurlTrendsDF = # FILL_IN"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\ncols = urlTrendsDF.columns\nsample = urlTrendsDF.first()\n\ndbTest(\"ET2-P-08-06-01\", True, \"hour\" in cols and \"parsedURL\" in cols and \"count\" in cols)\ndbTest(\"ET2-P-08-06-02\", 18, sample[\"hour\"])\ndbTest(\"ET2-P-08-06-03\", \"twitter.com\", sample[\"parsedURL\"])\ndbTest(\"ET2-P-08-06-04\", 159, sample[\"count\"])\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["## Exercise 3: Join New Data\n\nFilter out bad users."],"metadata":{}},{"cell_type":"markdown","source":["### Step 1: Import Table of Bad Actors\n\nCreate the DataFrame `badActorsDF`, a list of bad actors that sits in `/mnt/training/twitter/supplemental/badactors.parquet`."],"metadata":{}},{"cell_type":"code","source":["# TODO\nbadActorsDF = # FILL_IN"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\ncols = badActorsDF.columns\nsample = badActorsDF.first()\n\ndbTest(\"ET2-P-08-07-01\", True, \"userID\" in cols and \"screenName\" in cols)\ndbTest(\"ET2-P-08-07-02\", 4875602384, sample[\"userID\"])\ndbTest(\"ET2-P-08-07-03\", \"cris_silvag1\", sample[\"screenName\"])\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["### Step 2: Add a Column for Bad Actors\n\nAdd a new column to `tweetDF` called `maliciousAcct` with `true` if the user is in `badActorsDF`.  Save the results to `tweetWithMaliciousDF`."],"metadata":{}},{"cell_type":"code","source":["# TODO\ntweetWithMaliciousDF = # FILL_IN"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\ncols = tweetWithMaliciousDF.columns\nsample = tweetWithMaliciousDF.first()\n\ndbTest(\"ET2-P-08-08-01\", True, \"maliciousAcct\" in cols and \"id\" in cols)\ndbTest(\"ET2-P-08-08-02\", 950438954272096257, sample[\"id\"])\ndbTest(\"ET2-P-08-08-03\", False, sample[\"maliciousAcct\"])\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["## Exercise 4: Load Records\n\nTransform your two DataFrames to 4 partitions and save the results to the following endpoints:\n\n| DataFrame              | Endpoint                            |\n|:-----------------------|:------------------------------------|\n| `urlTrendsDF`          | `/tmp/urlTrends.parquet`            |\n| `tweetWithMaliciousDF` | `/tmp/tweetWithMaliciousDF.parquet` |"],"metadata":{}},{"cell_type":"code","source":["# TODO - FILL_IN"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\nurlTrendsDFTemp = spark.read.parquet(\"/tmp/urlTrends.parquet\")\ntweetWithMaliciousDFTemp = spark.read.parquet(\"/tmp/tweetWithMaliciousDF.parquet\")\n\ndbTest(\"ET2-P-08-09-01\", 4, urlTrendsDFTemp.rdd.getNumPartitions())\ndbTest(\"ET2-P-08-09-02\", 4, tweetWithMaliciousDFTemp.rdd.getNumPartitions())\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":38}],"metadata":{"name":"Custom-Transformations","notebookId":3440735389691550},"nbformat":4,"nbformat_minor":0}
